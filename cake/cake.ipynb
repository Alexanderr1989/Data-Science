{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8aba241",
   "metadata": {},
   "source": [
    "# Cake модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c22c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95656ddc",
   "metadata": {},
   "source": [
    "## Общая идея"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34dee7",
   "metadata": {},
   "source": [
    "Линейные модели хорошо предсказывают линейные зависимости, но они не справятся с нелинейной. Однако, любая кривая (или многомерная поверхность) при достаточном приближении также линейна. \n",
    "\n",
    "Суть модели в том, что мы \"разрежем\" наш датасет на кусочки по каждому признаку, которому захотим, т.е. на n-мерные параллепипеды, которые, как кусочки торта скормим уже обычным регрессиям (или любым другим моделям).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8208e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cake:\n",
    "    \n",
    "    def __init__(self, chunks = 2, model = LinearRegression(), features = False ):\n",
    "        # При нулевом и меньшем значении модель ломается, так что делаем защиту от дурака\n",
    "        if chunks < 1:\n",
    "            chunks = 1\n",
    "        #Правильней было бы назвать это \"глубина\". Chunks - то на сколько кусков модель разрежет КАЖДУЮ фичу \n",
    "        #(в итоге мы можем получить огромные значения - общее количество кусков равно параметр Chunk в степени\n",
    "        #количество признаков)\n",
    "        self.chunks = chunks \n",
    "        #Параметр показывает над какими именно признаками проводить операции. Если указано False (по умолчанию),\n",
    "        #то над всеми\n",
    "        self.features = features\n",
    "        #Признак отвечает за то, какая именно модель будет обучаться на каждом кусочке. Изначально, линейная регрессия,\n",
    "        #но вы можете поставить любую (например, логистическую)\n",
    "        self.model = model\n",
    "    \n",
    "#Функция создающая фектор. Она принимает вектор (строку нашей матрицы) и возвращает вектор где все \"номера кусков\"\n",
    "#на один больше. Номера признаков она не трогает\n",
    "    def make_second_vector__(self, first_vector):\n",
    "            \n",
    "        second_vector = []\n",
    "        for i in first_vector:\n",
    "            second_vector.append((i[0], i[1] + 1))\n",
    "        return second_vector\n",
    "    \n",
    "\n",
    "#Вспомогательная функция, которая поможет \"разрезать\" датасет.    \n",
    "    def chunking__(self, features, chunks):\n",
    "        df = self.X\n",
    "        self.matrix_dict = {}\n",
    "#Для начала нам надо получить словарь,  в котором будут следующие параметры \n",
    "#- номер признака, номер куска и число, определяющее значение признака в данном куске. \n",
    "#Запись в словаре вида (0, 1): 5 означает что значение начала второго куска первого признака равна 5 (не забываем, что\n",
    "#счёт начинается с нуля)\n",
    "        for i in range(len(self.features_list)):\n",
    "            for i2 in range(chunks):\n",
    "                chunk = 0\n",
    "                #Начало самого первого куска = -бесконечность\n",
    "                if i2 > 0:\n",
    "                    chunk = ( \n",
    "                    (df[self.features_list[i]].max() - df[self.features_list[i]].min() ) #Длина расброса значений\n",
    "                    / chunks ) * i2 + df[self.features_list[i]].min()\n",
    "                else:\n",
    "                    chunk = -np.inf\n",
    "                self.matrix_dict[(i,i2)] = chunk \n",
    "#Мы заполнили все промежуточные значения, теперь нам надо заполнить последние. Считаем, значения признаков (на тестовой вы-\n",
    "#борке) могут уходить в бесконечность, потому конец последнего куска будет равен np.inf\n",
    "        for i in range(len(self.features_list)):\n",
    "            #chunk = df[self.features_list[i]].max()\n",
    "            chunk = np.inf\n",
    "            self.matrix_dict[(i,chunks)] = chunk\n",
    "        del df #удаляем лишнюю переменную\n",
    "\n",
    "#Теперь нам понадобится сгенерировать матрицу, которая будет содержать всевозможные пары чисел (номер признака, номер куска)\n",
    "#Запись вида [(0,0), (1,0), ...] будет означать \"первый признак, первый кусок, второй признак и первый кусок и т.п.\"\n",
    "        self.matrix = []\n",
    "        matrix = []\n",
    "        g = list(range(chunks))\n",
    "        \n",
    "        matrix = list(product( g, repeat = len(self.features_list)) )\n",
    "        for i in matrix:\n",
    "            vector = []\n",
    "            for i2 in range(len(i)):\n",
    "                vector.append((i2, i[i2]))\n",
    "            self.matrix.append(vector)\n",
    "        \n",
    "        \n",
    "#Функция обучения модели. Кто бы мог подумать?        \n",
    "    def  fit(self, X,y):\n",
    "        #ATTENTION! АХТУНГ! ВНИМАНИЕ! Если соберётесь делать что-то подобное, используйте copy()! \n",
    "        #До этого я много часов потратил, чтобы понять почему модель работает из рук вон плохо, а при попытке засунуть в неё\n",
    "        #логистическую регрессию - выдаёт исключения и не хочет работать! Подробней будет описано ниже.\n",
    "        self.X = X.copy()\n",
    "        self.X = self.X.reset_index(drop=True)\n",
    "        self.y = y.copy()\n",
    "        self.y = self.y.reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        self.model_matrix = {}\n",
    "        #Настраиваем признаки по которым будем делить датасет\n",
    "        if self.features == False:\n",
    "            self.features_list = list(X.columns)\n",
    "        else:\n",
    "            self.features_list = self.features\n",
    "        #Используем функцию, которая разбивает датасет по кусочком, а точнее, создаёт индексы всех кучочков и словарь,\n",
    "        #в котором содержатся численные выражения этих самых индексов\n",
    "        self.chunking__( self.features_list, self.chunks)\n",
    "        \n",
    "        #Что если у нас не окажется данных в каких-то кусках, но они будут в тестовой выборке? Тогда мы не сможем исполь-\n",
    "        #зовать наши модели, и будем делать предсказания из общих оснований. Т.е. обучим модель на ВСЕЙ выборке и будем\n",
    "        #использовать ЕЁ предсказания\n",
    "        self.base_model = copy.copy(self.model)\n",
    "        self.base_model.fit(X,y)\n",
    "        \n",
    "        #Список для наших моделей\n",
    "        models = []\n",
    "        \n",
    "\n",
    "        #Теперь нам нужно обучить наши модели на кусочках нашего датасета. Для этого нам нужно извлечь сами кусочки.\n",
    "        for i in self.matrix:\n",
    "        #е - второй вектор, где все значения индекса куска больше на единицу\n",
    "            \n",
    "            e = self.make_second_vector__(i)\n",
    "            \n",
    "            values = []\n",
    "            for p in range(len(i)):\n",
    "                a = ''\n",
    "                if p < (len(i) - 1 ):\n",
    "                    a = ' and '\n",
    "                #В начале должен быть знак строго меньше, а в конце <=, иначе могут возникнуть пересечения кусков, что \n",
    "                #поломает всю модель\n",
    "                values.append(str(self.matrix_dict[i[p]]) + '<' + \n",
    "                              self.features_list[i[p][0]] + '<=' + str(self.matrix_dict[e[p]]) + a)\n",
    "            #цикл выше генерирует строку, которую мы пихаем в df.query чтобы получить нужный нам кусочек, а точнее его \n",
    "            #индексы\n",
    "            values = ''.join(values)\n",
    "            indexes = self.X.query(values).index\n",
    "            #Теперь, наконец, создаём тренировочную мини-выборку из одного кусочка для наших моделей. \n",
    "            X_train = self.X.iloc[indexes]\n",
    "            y_train = self.y.iloc[indexes]\n",
    "            \n",
    "            if not X_train.empty: #Нам вполне могут попасться кусочки без данных, это нужно учесть.\n",
    "                #Если так случилось, то вместо модели запоминаем пустое значение\n",
    "                \n",
    "                #Некоторые модели чувствительны к составу данных в целевом признаке, например, логистическая регрессия \n",
    "                #выдаст исключение, если там только нули или только единицы. А такое вполне возможно, так что используем\n",
    "                #try-except\n",
    "                try:\n",
    "                    this_model = self.model.fit(X_train, y_train)    \n",
    "                except:\n",
    "                    this_model = None\n",
    "            else: \n",
    "                this_model = None\n",
    "            model_matrix_append = { (tuple(i),tuple(e)) : copy.copy(this_model)}\n",
    "            #Тут как видите, я засовываю в матрицу не просто this_model, а copy.copy(this_model). Если так не делать, то\n",
    "            #определяя this_model заново, я буду перезаписывать все модели в матрице. Т.е. по задумке у меня на каждый\n",
    "            #кусочек данных своя модель, отвечающая именно за него, а до этого во всех кусках была ПОСЛЕДНЯЯ модель.\n",
    "            #Логично, что датасет работал хуже обычной регрессии, так как по сути использовал ту же регрессию, но обученную\n",
    "            #на малом куске данных!\n",
    "            del this_model\n",
    "            self.model_matrix.update(model_matrix_append)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "    #Теперь, когда мы обучили наши модели, нужно предсказывать. Так же как и в обучении, мы пройдёмся по всей матрице и\n",
    "    #индексами начал кусков и разобьём тестовый датасет на эти самые куски. Затем мы будем брать модель, обученную на \n",
    "    #аналогичных кусках и с помощью неё делать предсказание (только для этого куска)\n",
    "        \n",
    "        self.Xtest = X.copy().reset_index(drop=True)\n",
    "        #Если не дропать индексы, получится каша, предсказания будут перепутаны и всё такое.\n",
    "        self.predictions = []\n",
    "        predictions = []\n",
    "\n",
    "        for i in self.matrix: #Да-да, я копирую большую часть кода. Индусы могли бы мной гордиться.\n",
    "        #е - второй вектор, где все значения индекса куска больше на единицу. Как и в предыдущем коде.\n",
    "            \n",
    "            e = self.make_second_vector__(i)\n",
    "            \n",
    "            values = []\n",
    "            for p in range(len(i)):\n",
    "                a = ''\n",
    "                if p < (len(i) - 1 ):\n",
    "                    a = ' and '\n",
    "                values.append(str(self.matrix_dict[i[p]]) + '<' + \n",
    "                              self.features_list[i[p][0]] + '<=' + str(self.matrix_dict[e[p]]) + a)\n",
    "            #цикл выше генерирует строку, которую мы пихаем в df.query чтобы получить нужный нам кусочек, а точнее его \n",
    "            #индексы\n",
    "            values = ''.join(values)\n",
    "            \n",
    "            indexes = self.Xtest.query(values).index                \n",
    "            #Создаём тот кусочек данных, на которых мы будем предсказывать. \n",
    "            X_test = self.Xtest.iloc[indexes]\n",
    "            X_test = X_test.reset_index(drop=True)\n",
    "            \n",
    "            if not X_test.empty: #Нам вполне могут попасться кусочки без данных, это нужно учесть.\n",
    "\n",
    "                #Выбираем модель из матрицы\n",
    "                this_model = self.model_matrix[(tuple(i),tuple(e))]\n",
    "                \n",
    "                #Если модели нет, то используем обученную на всех данных, если есть, то из матрицы\n",
    "                if this_model != None:\n",
    "                    \n",
    "                    local_predictions = this_model.predict(X_test)\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    local_predictions = self.base_model.predict(X_test)\n",
    "\n",
    "            #Теперь создаём список из кортежей, где первый элемент кортежа инлдекс, а второй - предсказание. Добавляем\n",
    "            #к глобальному списку предсказаний\n",
    "                predictions += list(zip(indexes, local_predictions))\n",
    "                \n",
    "        #Выходим из цикла\n",
    "        \n",
    "        #Сортируем предсказания по индексу (т.е. первому элементу кортежа)\n",
    "        predictions.sort(key = lambda x: x[0])\n",
    "        \n",
    "        #превращаем наш список кортежей в нормальный список, который и будем возвращать.\n",
    "        for i in predictions:\n",
    "            self.predictions.append(i[1])\n",
    "            \n",
    "        self.predictions = np.array(self.predictions)\n",
    "        return self.predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
